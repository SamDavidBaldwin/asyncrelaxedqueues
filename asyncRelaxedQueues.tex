\documentclass[a4paper,USenglish]{lipics-v2021} % TODO: Re-add anonymous tag

\usepackage{algorithm,algpseudocode}
\algtext*{EndIf}
\algtext*{EndFor}

\theoremstyle{definition}
\newtheorem{construction}{Construction}

\newcommand{\red}[1]{\textcolor{red}{#1}} % TODO: Delete this before submission
\newcommand{\pluseq}{\mathrel{+}=}

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Relaxation for Efficient Asynchronous Queues}

\titlerunning{Asynchronous Relaxed Queues}

\author{Samuel Baldwin}{Bucknell University, USA}{}{orcid}{}

\author{Cole Hausman}{Bucknell University, USA}{}{[orcid]}{}

\author{Mohamed Bakr}{Bucknell University, USA}{}{ORCID}{}

\author{Edward Talmage}{Bucknell Univserity, USA}{elt006@bucknell.edu}{https://orcid.org/0009-0001-9108-6190}{}

\authorrunning{S. Baldwin, C. Hausman, M. Bakr, E. Talmage} %TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{Samuel Baldwin, Cole Hausman, Mohamed Bakr, Edward Talmage} 

\begin{CCSXML}
  <ccs2012>
   <concept>
       <concept_id>10003752.10003809.10010172</concept_id>
       <concept_desc>Theory of computation~Distributed algorithms</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
   <concept>
       <concept_id>10003752.10003809.10010031</concept_id>
       <concept_desc>Theory of computation~Data structures design and analysis</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
   <concept>
       <concept_id>10010147.10010919.10010172</concept_id>
       <concept_desc>Computing methodologies~Distributed algorithms</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
   </ccs2012>
\end{CCSXML}
\ccsdesc[500]{Theory of computation~Distributed algorithms}
\ccsdesc[500]{Theory of computation~Data structures design and analysis}
\ccsdesc[500]{Computing methodologies~Distributed algorithms} %Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm 

\keywords{Distributed Data Structures, Asynchronous Algorithms, Relaxed Data Types} %TODO mandatory; please add comma-separated list of keywords

\funding{Funding provided by Bucknell University}

%\acknowledgements{I want to thank \dots}%optional

%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\EventEditors{John Q. Open and Joan R. Access}
\EventNoEds{2}
\EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
\EventShortTitle{CVIT 2016}
\EventAcronym{CVIT}
\EventYear{2016}
\EventDate{December 24--27, 2016}
\EventLocation{Little Whinging, United Kingdom}
\EventLogo{}
\SeriesVolume{42}
\ArticleNo{23}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle

\begin{abstract}
We explore the problem of efficiently implementing shared data structures in an asynchronous computing environment.  We start with a traditional FIFO queue, showing that full replication is possible with a delay of only only two round-trip messages between invocation and response of each operation.  We argue that this is optimal, since any shorter delay would allow scenarios where an algorithm cannot determine correct return values.  We then turn our attention to improving performance.  Here, though we cannot improve the worst-case time per operation instance, we show that \emph{relaxation}, weakening the ordering guarantees of the Queue data type, allows most $Dequeue$ instances to return more quickly than the worst-case, giving a low amortized cost per instance.
\end{abstract}

%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%
\section{Introduction}

Shared data is a core aspect of modern computing.  Datasets are growing, local computational power cannot keep up, and data collection is continually growing more widespread.  In the naturally resulting geographically distributed systems, one of the primary concerns is accessing data that other members of the system may be accessing concurrently.  Without careful coordination, we can inadvertently use stale versions of the data, overwrite other users' work, or even corrupt the data.

Distributed data structures are a fundamental construct for allowing computing to spread across different machines which need to work together.  By specifying the interface of data operations users may call and the exact effects of those operations, we provide an abstraction layer that allows a programmer designing for a distributed system to not worry about the details of coordinating and maintaining data.  Instead, they can focus on the particular application in which they are interested.  In co-located parallel computation, hardware can exist to provide shared access to memory.  In geographically distributed system, we must build that abstraction layer, considering all the possible oddities of concurrency, messages, delays, and different views of the data.  We focus on optimizing the interactions with these system elements, and provide the developer with an efficient tool which provides well-specified guarantees.

Our primarily concern in this work is to minimize the delays users experience when working with shared data.  Past work has given tight or nearly tight bounds on the possible implementations of a variety of data structures \cite{Kosa99,WangTalmageLeeWelch18} in partially synchronous systems, were there are bounds on the real time which messages between participating processes may take to arrive and processes know and can depend on those bounds.  However, real-world systems do not generally have reliable bounds on timings, so these algorithms are difficult to realistically implement.  As a next step towards practical implementations of efficient distributed data structures, we here consider an asynchronous model of computation.  In this model, processes cannot rely on any local measure of time, or on message delays carrying any information about when remote operations occur, or their order.  Instead, we construct logical timestamps and use them to agree on the order in which operations take place.

This paper contains two results.  First, we present an algorithm for a standard FIFO queue in a fully asynchronous message passing model. While there exist existing solutions for queues in this model \cite{Lynch96}, to our knowledge, none offer full replication, which our solution does.  Replication is important for a few reasons.  First, it enables fault-tolerance, since we do not lose data if a participating system crashes.  We know that asynchronous, fault-tolerant queues are impossible to implement \cite{FischerLynchPaterson85, Herlihy91}, so we cannot immediately proceed to adding fault tolerance to this algorithm.  There are other benefits of replication, though, such as data locality and not relying on a central coordinating process, which may be subject to malicious control.  Our main purpose in relaxation is to support our second result, which may be able to avoid previous impossibility results and lead to future fault-tolerant data structures.

Our second result builds on the notion of \emph{relaxing} a data type: weakening the guarantees of the data type specification \cite{HenzingerKirschPayerSezginSokolova13}.  We can then use this weakening to achieve better performance than we can for a basic data type.  Following Talmage and Welch \cite{TalmageWelch14}, we consider a version of a Queue datatype that allows $Dequeue$ to return not just the single oldest element, but any sufficiently old element in the structure.  This allows us to determine return values for most $Dequeue$ invocations without waiting for any communication, thus bringing the amortized cost of $Dequeue$ far below the minimum possible for an unrelaxed queue.  We provide an algorithm doing this, building on our first algorithm.  This algorithm is the first to implement a relaxed queue in an asynchronous system and proves that the performance benefits Talmage and Welch \cite{TalmageWelch14} found in partially synchronous systems are also possible in asynchronous systems.  While the relaxed queue does not provide the same ordering guarantees as a FIFO queue, in many concurrent executions, the behavior will be indistinguishable.  If a process dequeues, for example, the third oldest element in the queue, this appears similar (at least at that time), to an execution of an unrelaxed queue where two other processes concurrently dequeued the two older elements, as this process will still dequeue the third oldest element.

Another intriguing property of relaxed queues is that they are not as subject to the impossibility results for asynchronous, fault-tolerant systems \cite{ShavitTaubenfeld16,TalmageWelch19}.  Thus, it may well be possible to extend our result in the future to asynchronous, fault-tolerant implementations.  This would be a major step forward, since the impossibility of implementing queues, and most other useful data types severely limits the capabilities of distributed systems.  This paper is a step on the road toward that goal, and we are excited to pursue such implementations.

%and therefore the ability to extend to fault tolerant systems. Our algorithm utilizes vector clock timestamps that allow individual processes to hold some view of the time steps of other processes have taken at points of communication. Using this technique to timestamp invocations of the two operations of the queue, then linearizing all of the queue invocations based on the ascending lexicographic order of these timestmps creates a valid permutation that meets the specifications of a FIFO Queue.  The goal of establishing this queue method is to design a system with full replication. There are existing systems that handle the asynchronous queue (server client model), but are incapable of replication. With replication, and (INSERT THE CITATION TO THE PAPER HERE) we hope that this queue algorithm can be used in fault tolerant systems in the future.

%%%%%%%%%%%%%%%
\subsection{Related Work}

TODO

% Lamport: Linearizability, data structure implementations
% ABD: Register implementations in asynchronous systems

% Relaxation:
% Afek et al. Quasi-linearizability
% Henzinger et al. Quantitative relaxation
% Talmage Welch '14 improving data structure performance

% Computational power
% FLP, Herlihy
% Shavit Taubenfeld/TalmageWelch19: lower CNs for relaxed queues

%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%
\section{Model and Definitions}

%%%%%%%%%%%%%%%
\subsection{Asynchronous System Model}
We assume a fully asynchronous message passing model of computation.  The system contains a set of $n$ processes $\Pi = [p_0, \dots , p_{n-1}]$ modeled as state machines, and a set of $n$ users, one for each process.  Each state machine accepts two types of inputs: the corresponding user may \emph{invoke} an operation, or the machine may \emph{receive} a message.  Each of these will trigger a handler.  Each process, in its handlers, may perform local computation, or perform one of of two external actions: it may \emph{respond} to its user or \emph{send} a message to another process.  The state machines are time-free, meaning that their output is only described through their input and state transitions without any specific time bound.  We also assume no processes fail, instead performing exactly as per the state machine specification.  

We assume all inter-process communication is reliable, so when a process sends a message, that message will eventually be received at its destination process exactly once.  Additionally, we assume communication channels between processes are First-In, First-Out.  That is, if process $p_i$ sends message $m_1$ to process $p_j$, then sends message $m_2$ also to $p_j$, then $p_j$ will receive $m_1$ before it receives $m_2$.  This assumption does not reduce the generality of our results, as we can implement such an ordering guarantee by attaching a sequence number to each message and buffering incoming messages at each process.  We store out of order messages in this buffer until all previous messages are received, then receive the buffered message.

%TODO: expand definition of reliable channels to mention messages arrive in finite time

While there is no notion of time available to processes in the system, we establish a notion of the time cost of our algorithms by considering that they run in real time.  There are no upper or lower bounds on either local computation time or the time between when a message is sent and received.  However, we can measure an algorithmic time, from outside the system, by expressing it in terms of the number of sequential messages which must occur in that duration \cite{AttiyaWelch04}.  That is, in a particular run, we define the parameter $d$ as the real time duration of the longest time between sending and receiving any one message.  We express algorithmic time in terms of $d$, by considering how many messages the algorithm sends and receives, with each send necessarily occurring after the receive of the previous message.  Since each of those delays may be the largest in that run, this gives a measure of algorithmic runtime without bounding real time message delays.  

%%%%%%%%%%%%%%%
\subsection{Data Type Definitions}

We first state the definition for a standard FIFO queue abstract data type, then that for the relaxed queue we study here.  We state abstract date type (ADT) specifications in two parts: First, a list of operations the user can invoke, with argument and return types, expressed as $OP(arg,ret)$.  We use $-$ to indicate when a function takes no argument or returns no value.  Second, we define the set of legal \emph{instances} of these operations.  An operation instance is an invocation-response pair, noting that these are significantly separate events in a distributed setting.  We assume that each process' user cannot invoke an operation until it has received a response to its last previous invocation.   

To simplify the statement of our definitions, we assume that each argument to $Enqueue$ is unique.  One way to achieve this practically would be a simple abstraction layer that adds a (logical) timestamp to each value before it is passed to the data structure.  We also define the notion of \emph{matching}: A $Dequeue$ instances matches an $Enqueue$ instance if it returns that $Enqueue$'s (unique) argument.  We will use the special character $\bot$ to represent an empty queue.

\begin{definition} A \emph{Queue} over a set of values V is a data type with two operations:
  \begin{itemize}
  \item $Enqueue(val,-), val \in V$ 
  \item $Dequeue(-, val), val \in V \cup \{\bot\}$ 
  \end{itemize}
  
  The empty sequence is legal.  For any legal sequence $\rho$ of instances of queue operations and $val \in V$, (1) $\rho \cdot Enqueue(val,-)$ is legal, (2) $\rho \cdot Dequeue(-,val)$ is legal iff $Enqueue(val, -)$ is the first unmatched $Enqueue$ instance in $\rho$, and (3) $\rho \cdot Dequeue(-, \bot)$ is legal iff every $Enqueue(val, -)$ in $\rho$ is matched.
\end{definition}

We can now formally define the relaxed queue we implement in this paper.  Intuitively, each $Dequeue$ instance can return one of the $k$ oldest elements in the queue.  If $k=1$, this is the same as the FIFO Queue defined above.

\begin{definition} A \emph{Queue with $k$-Out-of-Order relaxed $Dequeue$}, or an \emph{$k$-Out-of-Order Queue}, over a set of values $V$ is a data type with two operations:
  \begin{itemize}
  \item $Enqueue(val,-), val \in V$
  \item $Dequeue(-,val), val \in V$
  \end{itemize}

  The empty sequence is legal.  For any legal sequence $\rho$ of instances of $k$-out-of-order queue operations and $val \in V$, (1) $\rho \cdot Enqueue(val,-)$ is legal, (2) $\rho \cdot Dequeue(-,val)$ is legal iff $val$ is the argument of one of the first $k$ unmatched $Enqueue$ instances in $\rho$, and (3) $\rho \cdot Dequeue(-,\bot)$ is legal iff there are fewer than $k$ unmatched $Enqueue$ instances in $\rho$.
\end{definition}

We are interested in implementations which satisfy \emph{linearizability} \cite{HerlihyWing90}.  Linearizability is a consistency condition which describes how concurrent executions of a data structure relate to the sequential specification of the ADT the structure implements.  Specifically, linearizability requires that operation response values be such that there is a total order of all operation instances in the execution which is legal according to the ADT and which respects the real-time order of instances which do not overlap in real time.  That is, if one instance returns before, in real time, another's invocation, that one which returned first must be ordered first.  This condition provides behavior that matches that we intuitively expect systems to have, disallowing inversions in real time.  Linearizability also has the advantage of being composable, meaning that running multiple linearizable objects in the same execution will necessarily produce an overall linearizable execution.  Linearizability is the strongest consistency condition we might satisfy, so our upper bounds translate to any other consistency condition one might use.

% TODO: Define run, timed run, admissible (must be infinite), linearization, 

%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%
\section{Asynchronous FIFO Queues}

%%%%%%%%%%%%%%%
\subsection{Description}

TODO: Write high-level description of algorihtm. (broadcasts, confirmations/conflists, local copy of the queue and local execution, etc.).  Don't put a lot of detail here, as we'll expand on those below.

%%%%%%%%%%
\subsubsection{Vector Clocks}

Each process stores a vector clock timestamp that holds its local view of the clocks at all processes.  We will refer to the local vector clock at process $p_i$ as $v_i$.  Each process' vector clock is an array of size $n$ that is initially 0 at all indices.  When any process $p_i$ invokes $Enqueue$ or $Dequeue$, $p_i$ will increment index $i$ in its local clock, $v_i[i]$.  Processes also update the local view of the vector clocks when they receive a message containing a timestamp from another process.  Each invocation of $Enqueue$ or $Dequeue$ sends a timestamp, as well as other messages our algorithm uses for coordination.  To update its local vector clock, the receiving process compares values at corresponding indexes of the local clock and received timestamp, and sets its local clock at that index to the larger of the two values.  By adjusting each of these indices, this guarantees that a local clock will have the largest of the two indices at all indices in the local clock, recording its knowledge of the previous remote event for all later timestamps.

We define two orders on timestamp vectors.  For any two vector timestamps $v_i$ and $v_j$, we say $v_i$ is \emph{strictly smaller} than $v_j$, denoted $v_i \prec  v_j$, if $v_i[x] < v_j[x],\forall x \in [0, \dots, n-1]$.  If this is not true, there is some index $k$, where the vectors first differ.  That is, for $x = 0$ to $k-1$, $v_i[k] < v_j[x]$ but $v_i[k] \geq v_j [k]$.  Then we say that $v_i$ is \emph{lexicographically smaller} than $v_j$ and write $v_i << v_j$.  Notice that $v_i \prec v_j$ implies that $v_i << v_j$ but not the opposite.

Each process maintains a local augmented minimum priority queue keyed on lexicographic timestamp order.  This priority queue can perform three operations: $insert(value, v_{clock})$, $get(position)$, and $remove(value)$.  The $insert$ operation adds the value to the queue with $v_{clock}$ as a priority.  The $get(position)$ function allows the user to peek into the element at the passed position (returns the element with the $position$th-lowest timestamp) without removing a value from the queue.  The $remove(value)$ function removes the specific value passed to it from the queue, independent of priority order.  Ordering elements in FIFO order is not a straight-forward task in a distributed setting since defining which invocation happened first is well defined for concurrent operation instances.  We will use lexicographic timestamp order for our linearization, so using a priority queue keyed on lexicographic timestamp order allows us to ensure each process' local view matches that of the linearization order.

%%%%%%%%%%
\subsubsection{Confirmation Lists}
The main algorithmic idea that allows us to handle asynchrony is a structure we call \emph{Confirmation Lists}.  Each process uses these lists to track acknowledgments from other processes for a given $Dequeue$ instance.  When aprocess $p_j$ receives a $Dequeue$ request message from another process $p_i$, $p_j$ will either declare that $Dequeue$ instance $unsafe$ or $safe$ depending on whether $p_j$ has or has not, respectively, invoked a $Dequeue$ intance with smaller (lexicographically)\footnote{\textcolor{red}{TODO: make sure I got the right ordering here}} timestamp.  $p_j$ will then send a its response to all processes, declaring its view of the $Dequeue$ instance as $safe$ or $unsafe$. These $safe$/$unsafe$ messages also contain the invoking process's id, the responding process' id, and the invocation's vector clock timestamp.  \textcolor{red}{TODO: mention invoation timestamps earlier somewhere}  The idea is that, if $p_j$ is already dequeueing, it will mark $p_i$'s $Dequeue$ as $unsafe$ and broadcast this, to ensure that all processes execute $p_j$'s $Dequeue$ instance on their local copy before $p_i$'s $Dequeue$ instance.

To track these $safe$/$unsafe$ responses, when a process first receives information about a $Dequeue$ invocation, it creates a \emph{confirmation list} object, keyed to the timestamp of the $Dequeue$ invocation and stores that object.  This object contains the timestamp that distinguishes it from other confirmation lists, then an array of length $n$, to track confirmations from each process.  A process can learn about an invocation by invoking a $Dequeue$ itself, from a $Dequeue$ request message directly from the invoking process, or from a third-party process' response message to a $Dequeue$ invocation.  A confirmation list's array is empty at creation, except for a $safe$ in the position corresponding to the invoking process.  The list fills as the process receives $safe$/$unsafe$ responses to the $Dequeue$ request, placing each in the position corresponding to the responding process.  If a process is creating a confirmation list on receiving a response before the initial $Dequeue$ request, it can place that response in the position corresponding to the sender immediately on creating the confirmation list.

If process $p_j$ is storing a confirmation list for a $Dequeue$ instance invoked at process $p_i$, when the confirmation list's array has no empty cells because it has received responses from all processes, $p_j$ can locally execute that $Dequeue$ instance, applying it to its own local copy of the queue.  To ensure that it removes the same element as all other processes, $p_i$ counts the number of $unsafe$ responses stored in the instance's confirmation list and uses that as the $position$ argument to a $get$ call.  Each $unsafe$ corresponds to another, concurrent $Dequeue$ instance which has a lexicographically smaller timestamp, and which thus will be linearized earlier.  Thus, the algorithm uses the $get$ function to leave the elements those preceding $Dequeu$ instances wll return.

At the point of locally executing a $Dequeue$ instance via the confirmation list structure, any other confirmation lists for $Dequeue$ instances with lexicographically later timestamps must be updated, since they no longer need to save an element for the already-executed instance.  The invoking process for the executed $Dequeue$ instance would have responsed with $unsafe$ to any $Dequeue$ requests during the period between when invoking $Dequeue$ and when it locally executes it.  When $p_j$ locally executes the invocation, it removes the element that the process is effectively reserving with the $unsafe$ responses. Therefore, we can change to $safe$ all responses $p_i$ sent as $unsafe$ while its $Dequeue$ instance was pending.  To do this, $p_j$ will update confirmation lists stored for $Dequeue$ instances with lexicographically later timestamps, changing any $unsafe$ values in the position corresponding to $p_i$ to $safe$.  This updating stops at the next confirmation list for a $Dequeue$ instance invoked at process $p_i$, as any $unsafe$ responses $p_i$ sent after that point will be because this next $Dequeue$ instance was pending, instead of because of the original instance.

%%%%%%%%%%%%%%%
\subsection{Algorithm}

\begin{algorithm}
  \caption{Code for each process $p_i$ to implement a Queue, Handlers for $Enqueue$}\label{alg:fifo}
  \begin{algorithmic}[1]
    \Function{Enqueue}{$val$}
      \State $EnqResponseCount = 0$ \Comment{Count responses to this invocation}
      \State $Enq_{ts} = incrementTS()$ \label{fifoline:enqTS} \Comment{Increment local vector clock, read instance's timestamp}
      \State send $(EnqReq, val, Enq_{ts}, i)$ to all processes\label{fifoline:sendEnqReq}
    \EndFunction

    \Function{Receive}{$EnqReq, val, inv, Enq_{ts}$} from $p_{inv}$
      \State $updateTS(Enq_{ts})$ \label{fifoline:enqReqTSUpdate} \Comment{Update local vector clock by invocation's timestamp}
      \State $lQueue.insertByTS$($val, inv, Enq_{ts}$)\label{fifoline:executeEnq} \Comment{Locally execute the $Enqueue$ instance}
      \State send $(EnqAck, i)$ to $p_{inv}$ \label{fifoline:sendEnqAck} \Comment{Acknowledge receipt of the invocation}
    \EndFunction

    \Function{Receive}{$EnqAck, j$ from $p_j$}
      \State $EnqResponseCount \pluseq 1$
      \If {$EnqResponseCount == n$} 
        \Return $EnqResponse$ to user\label{fifoline:enqReturn} 
      \EndIf
    \EndFunction
    \Statex
    \Statex \emph{(continues below)}
    \algstore{fifoLines}
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Algorithm~\ref{alg:fifo} continued: Handlers for $Dequeue$}
  \begin{algorithmic}[1]
    \algrestore{fifoLines}
    \Function{Dequeue}{}
      \State $Deq_{ts} = incrementTS()$ \Comment{Increment local vector clock, read instance's timestamp}\label{fifoline:deqTS}
      \State $activeDeqTS = Deq_{ts}$ \Comment{Track active $Dequeue$ instance's timestamp}
      \State send $(DeqReq, Deq_{ts}, i)$ to all processes \label{fifoline:sendDeqReq}
    \EndFunction

    \Function{Receive}{$(DeqReq, Deq_{ts}, inv)$ from $p_{inv}$}
    \State $updateTS(Deq_{ts})$ \label{fifoline:deqReqTSUpdate}
      \If{$Deq_{ts}$ is not in $PendingDequeues$}
      \State $PendingDequeues.insertByTs(createList(Deq_{ts}, p_{inv}$))\label{fifoline:savePendingDeq}
      \EndIf
      \If{$activeDeqTS$ is not $null$ and $activeDeqTS \prec Deq_{ts}$} \Comment{If we have a preceding active $Dequeue$ instance, indicate $unsafe$, else indicate $safe$} \Comment{\red{This will collapse to a $DeqAck$ if I'm right about safe/unsafe being redundant}}
        \State $safetyFlag = Unsafe$
      \Else
        \State $safetyFlag = Safe$
      \EndIf
      \State send $(safetyFlag, Deq_{ts}, p_{inv}, i)$ to all processes \label{fifoline:sendSafetyFlag}\label{fifoline:sendDeqAck}
    \EndFunction

    \Function{Receive}{$(safetyFlag, Deq_{ts}, p_{inv}, j)$ from $p_j$}
      \If{$Deq_{ts}$ not in $PendingDequeues$}
        \State $PendingDequeues.insertByTs(createList(Deq_{ts}, p_{inv}))$
      \EndIf
      \State $propagateEarlierResponses(PendingDequeues,j)$ \Comment{\red{I think this goes away with safe/unsafe}}
      \For{($index, confirmationList)$ in $PendingDequeues$} \Comment{Add this response to confirmation list; locally execute any $Dequeue$ instances with all responses.} %TODO: This comment is too wordy, but the loop is somewhat confusing since it's doing two things.  Perhaps just describe carefully above.
      \If{$confirmationList.ts == Deq_{ts}$}
          \State $confirmationList.responses[j] = safetyFlag$
        \If{$confirmationList.full()$ and not $confirmationList.handled$}\label{fifoline:fullConfList}\Comment{\red{TODO: Make $handled$ go away}}
          \State $confirmationList.handled = True$ \Comment{\red{Why would a handled Dequeue still be in confirmationLists?  Once we handle it, can't we remove it?}}
          \State $ret = lQueue.deqByIndex(index)$\label{fifoline:chooseDeqValue} \Comment{Locally execute the $Dequeue$ instance, leaving elements for prior pending instances} 
          \State $updateUnsafes(PendingDequeues, index)$ \Comment{\red{What happens here when safe/unsafe is gone?}}
          \If{$p_i == p_{inv}$} \Return $ret$ to user \label{fifoline:deqReturn} \EndIf \Comment{The invoking process responds to the user}
        \Else\ break
        \EndIf
      \EndIf 
      \EndFor
      \EndFunction
      \Statex
      \Statex \emph{(continues below)}

    \algstore{fifoLines}  
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Algorithm~\ref{alg:fifo} continued: Helper functions}
  \begin{algorithmic}[1]
    \algrestore{fifoLines}
    
    \Function{propagateEarlierResponses}{$PendingDequeues, j$}
    \For{$row$ from $len(PendingDequeues) - 1$ down to $1$} \Comment{If a later-timestamp instance receives a $DeqAck$ from $p_j$, then any ealier-timestamp processes do not need to wait for a $DeqAck$ from $p_j$.} 
%    \For{$col$ from $0$ to $n-1$} \Comment{The process which sent the $DeqAck$}
      \State $response = PendingDequeues[row].responses[j]$ \Comment{\red{Cole/Sam: Please confirm that using $j$ this way is correct.}}
      \If{$response$ is not empty and $PendingDequeues[row-1].responses[j]$ is empty}
        \State $PendingDequeues[row-1].responses[j] = response$
      \Else\ break
      \EndIf
    \EndFor
    \EndFunction
%
    \Function{updateUnsafes}{$PendingDequeues, startIndex$} \Comment{When locally executing an instance, no longer need to set aside values for it, so update count for all later pending instances}
      \State $invoker = PendingDequeues[startIndex].invoker$
      \For{$index = startIndex$ upto $n$} \Comment{\red{Cole/Sam:Didn't this need to break if we saw another instance from the same process?}}
      \State $PendingDequeues[index].responses[invoker] = 1$  
      \EndFor
    \EndFunction
%
    \Function{updateTs}{$v_j$}
      \If{$v_j$ is empty}
        \State $v_i[i] \pluseq 1$
      \Else
        \For{$k = 0$ up to $n - 1$}
          \State $v_i[k] = max(v_i[k], v_j[k])$
        \EndFor
      \EndIf
      \State return $v_i$
    \EndFunction
%
    \Function{createList}{$Deq_{ts}, p_{invoker}$}
      \State create a new confirmation list $ConfirmationList$
      \State $ConfirmationList.responses = [0,0,...,0]$ \Comment{A list of $n$ 0s}
      \State $ConfirmationList.ts = Deq_{ts}$ \Comment{A vector clock timestamp}
      \State $ConfirmationList.handled = False$ \Comment{True once locally executed}
      \State $ConfirmationList.invoker = p_{invoker}$ \Comment{The invoking process of the Dequeue}
      \State return $ConfirmationList$
    \EndFunction
  \end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%
\subsection{Correctness}
In order to prove that this algorithm correctly implements the queue specification, we will consider an arbitrary, admissable timed run $R$.  We will first prove that all invocations have a matching return, so that we have complete operation instances.  We can then construct a total order of all operation instances in $R$ and prove that it respects real time order, making it a valid linearization, and is legal by the specification of a queue.  The core of the proof is in proving that when any process removes a particular $Dequeue$ instance's return value from its local replica of the queue, it deletes the same value as every other process does.  This means that all replicas will undergo the same series of operations, and allows us to prove that they continue removing the correct values.  To ensure that all processes remove the same element, we use the $safe$/$unsafe$ flags to leave elements for any $Dequeue$ instances which linearize earlier, but which that process is not ready to execute on its replica.

\begin{lemma}
  In $R$, every invocation has a matching response.
\end{lemma}

\begin{proof}
When a user invokes an operation at process $p_i$, the algorithm sends an $EnqReq$ message containing the invocation to all processes on line~\ref{fifoline:sendEnqReq} or \ref{fifoline:sendDeqReq}, depending on the operation invoked.  Each process will receive that message in finite time by our assumption of reliable channels and send back an $EnqAck$ for an $Enqueue$ invocation on line~\ref{fifoline:sendEnqAck} or a safety flag for a $Dequeue$ invocation on line~\ref{fifoline:sendSafetyFlag}, as appropriate.  Each of those responses will arrive in finite time, and when $p_i$ receives all $n$ of them, it will generate a matching return on line~\ref{fifoline:enqReturn} for $Enqueue$ or line~\ref{fifoline:deqReturn} for $Dequeue$.
\end{proof}

Each process reads its vector clock at each invocation, on line~\ref{fifoline:enqTS} or \ref{fifoline:deqTS}, and associates that clock value with that invocation throughout the algorithm.  We will refer to this clock value as the \emph{timestamp} of the instance containing that invocation.

\begin{construction}
  Let our linearization $\pi$ be the increasing lexicographic order of all operation instances in $R$.
\end{construction}

\begin{lemma}
  $\pi$ respects the real time order of non-overlapping instances.
\end{lemma}
\begin{proof}
  Proof by contradiction. Let there be two non-overlapping operation instances $op_1$ and $op_2$, where $op_1$ returns prior to $op_2$'s invocation in real time.  Assume, for the sake of contradition, that $op_2$ is prior to $op_1$ in $\pi$.
  
  Given that $op_1$ occurs before $op_2$ in real time, and that the two operation instances are non-overlapping, $op_2$'s timestamp must be larger, lexicographically, than $op_1$'s.  This follows from the fact that $op_1$ will not return until it receives a response from every process, including $op_2$'s invoking process (lines~\ref{fifoline:enqReturn}, \ref{fifoline:deqReturn}.  This means that $op_2$'s invoking process received the invocation for $op_1$ and updated its timestamp (line~\ref{fifoline:enqReqTSUpdate} or \ref{fifoline:deqReqTSUpdate}) before it invoked $op_2$, so $op_2$'s timestamp will be totally ordered after $op_1$'s, implying it is also lexicographically after.
\end{proof}

Next, we consider how each process locally executes each operation instance.  We show that all processes do so in the same way, meaning that their replicas of the data structure move through the same sequence of states, and thus continue to behave properly.

\begin{lemma}
  When a process locally executes any $Dequeue$ instance $op$, it has previously either locally executed any instance with a lexicographically smaller timestamp or has it in $pendingDequeues$.
\end{lemma}

\begin{proof}
  By line~\ref{fifoline:fullConfList}, $p_i$ will not locally execute $op$ until it has received a $DeqAck$ for $op$ from every process.  But when any process sends a $DeqAck$, on line~\ref{fifoline:sendDeqAck}, it has updated its timestamp to be larger than $op$'s timestamp.  Thus, any instance invoked at that process after that point will have a totally, and thus lexicographically, larger timestamp than $op$.  This means that any instance $op'$ with a lexicographically smaller timestamp than $op$ must have been invoked before its invoking process sent a $DeqAck$ for $op$.  By FIFO message order, that instance's request message, sent on line~\ref{fifoline:sendEnqReq} or \ref{fifoline:sendDeqReq}, would have arrived at $p_i$ before the $DeqAck$ for $op$ from that process, and $p_i$ would have either locally executed $op'$ on line~\ref{fifoline:executeEnq} if it was an $Enqueue$ instance, or put it in $pendingDequeues$ on line~\ref{fifoline:savePendingDeq} if it was a $Dequeue$ instance.  If it was a $Dequeue$ instance, the only time $p_i$ could remove $op'$ from $pendingDequeues$ is in lines~\ref{fifoline:fullConfList}-\ref{fifoline:deqReturn}, when $p_i$ locally executes $op'$.  Thus, when $p_i$ locally executes $op$, it must have already either locally executed $op'$, or have it in $pendingDequeues$, as claimed.
\end{proof}

This means that during local execution of a $Dequeue$ instance, each process already has the arguments of all earlier-linearized $Enqueue$ instances in its replica of the queue, and has either taken out the return values of all earlier-linearized $Dequeue$ instances, or has those instances in $pendingDequeues$, and can thus correctly count how many elements it should leave in the queue replica for those prior instances to return, and chooses and returns the correct value on line~\ref{fifoline:chooseDeqValue}.

\begin{lemma}
  $\pi$ is a legal sequence by the specification of a FIFO queue.
\end{lemma}

\begin{proof}

\end{proof}

\begin{lemma}
  The local views of the queues are equivalent at the time of $Dequeue$.
\end{lemma}

\begin{proof}
  For this algorithm, we will consider the entire history of the execution, such that all elements that were at one point in the queue are considered a part of the prefix of the queue. Once an element is dequeued, that element remains a part of the prefix, but no longer can be accessed, and is marked unavailable for dequeues.
  
Though processes may be in different local states with respect to active enqueues that they may have not recieved, the prefix of all local views up to the timestamp of the local execution of a dequeue will be the same. This is due to the fact that there is an agreed upon order for sorting the enqueues, and as such all processes will agree on the order. Furthermore, at the time of a dequeue, we can confirm that a given process will have heard of all prior enqueues. This is due to the fact that in order to locally execute a dequeue, a given process will have to have heard all messages with a lexicographically smaller timestamp to that dequeue, via the Confirmation List structure. This is due to the fact that in order to complete a dequeue execution, the Confirmation List strucuture must be filled, meaning that it must have recieved a message responding to the invocation from every other process in the system. Given that a message can only be acted upon in the case that it is the oldest unactioned message in the communication history between the two processes, we can be certain that there are no prior inter-process messages from a process which has a filled index in a confirmation list. The nature of the communication between processes being FIFO results in the knowledge that there cannot have been earlier messages that have gone unrecieved if a later message has been recieved and therefore, if a process is actively completing a dequeue invocation, at the point of that local execution, every message with a smaller timestamp has been received. Thus, we can formally state that at such a point, the prefix of all messages and queue states are equal across all processes.
\end{proof}

\begin{lemma}
In order to dequeue an element, there must be a corresponding $Enqueue$.
\end{lemma}

\begin{proof}
Proof by contradiction. Assume that some process invokes a dequeue, receives responses and locally executes, removing some element that was not enqueued.  That element, by the algorithm definition, must be held within the local view of the queue. This is because dequeues are accessed via the index provided by the unsafes. Therefore, the element that was dequeued by the process must exist in the local queue. The only way that can occur if the element was enqueued.  Additionally, when removing an element from the queue, it is important to not remove elements with a larger timestamp that the dequeue. In the case in which there is a dequeue request and a timestamp where $dqts < eqts$, and the dequeue executes locally, given we cannot linearize the enq before the deq, we must return $\bot$ and not the element. This behavior is defined in the algorithm, and allows us to establish a specified order.
\end{proof}

\begin{lemma}
Elements that are dequeued are unique. (No double dequeue.)
\end{lemma}

\begin{proof}
Given that at the time of execution we have proved that the head of the list of enqueues, combined with the algorithm definition guaranteeing that the local conf list of all processes contains the same information, we can state that all processes at the time of local execution will act on the same information to select an index, and therefore will select the same index. Given that, we can state that they will remove the same element from the queue, regardless of the time of execution locally.
\end{proof}

\begin{theorem}
  Algorithm~\ref{alg:fifo} correctly implements a FIFO queue.
\end{theorem}

\begin{proof}
Based on the information of the lemmas, we can conclude that the algorithm definition for the queue follows the specifications for the ADT. Enqeues have corresponding dequeues, no double dequeues, we can linearize by timestamp such that the first element in the queue is always removed first.
\end{proof}

  

%%%%%%%%%%%%%%%
\subsection{Complexity}

%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%
\section{Asynchronous Out-of-Order Queues}

%%%%%%%%%%%%%%%
\subsection{Description}

The asynchronous out of order queue is a relaxation of the existing queue structure that relaxes the specification that the queue can only return the first $Enqueue$ instance in $\rho$ that does not already have a matching $Dequeue(-, val)$ in $\rho$. Instead, it allows for the return of any of the first $K Enqueue$ instances in $\rho$ that do not already have a mathing $Dequeue(-, val)$ in $\rho$. 

The K-OOO queue also adds the behavior of prior claiming of elements for a system we will be referring to as "fast dequeueing". The existing behavior for dequeuing will be refered to as a "slow dequeue", which still utilizes the same behavior of Confirmation Lists to replicate information across the network. At points where processes share information however, there is additional behavior developed that allows processes to mark values in the queue that only they will be able to access, that will require no communication to be dequeued. At the point of execution of a "slow dequeue" if a process has a value that is marked to be dequeued by by that process, and the value that is marked is within the bounds of K, that it is one of the first K values in the queue structure, that value can be removed immediately without establishing a Confirmation List and requiring rounds of inter-process communication. Processes that are not the process that has "claimed" the value will additionally view that at the point of the slow dequeue, there is a labeled element for a fast dequeue from the same process, and remove both values from their local view of the queue, in order to ensure the same veiw of the structure. 

The process of labeling is done by guaranteeing that processes have the same view of the queue at certain points, and utilizing this similar view and invariant calculations to collectively agree on which processes claim which values. Given the processes agree on the ordering of values within the queue, the labeling of the elements and the ordering of slow dequeues, we can prove that fast dequeues do not disrupt the existing behavior of the Queue ADT. 
%%%%%%%%%%%%%%%
\subsection{Algorithm}

\begin{algorithm}
  \caption{Code for each process $p_i$ to implement a Queue with out-of-order k-relaxed \textit{Dequeue}, where $k \geq n$: Handlers for $Enqueue$}\label{alg:relaxed}
  \begin{algorithmic}[1]
    \Statex \Comment{\red{Is this entirely the same as for unrelaxed queues? I know I said to keep it, but I'm second-guessing whether we should include it if it's in a separate algorithm block and completely unchanged.}
}
    \Function{Enqueue}{$val$}
      \State $EnqCount = 0$
      \State $incrementTS(v_i)$
      \State $Enq_{ts} = v_i$
      \State send $(EnqReq, val, i, Enq_{ts})$ to all processes
    \EndFunction

    \Function{Receive}{$EnqReq, val, j, Enq_{ts}$} from $p_j$
      \State $updateTS(v_i, Enq_{ts})$
      \State $lQueue.insertByTS$($val, j, Enq_{ts}$)
      \For{$confirmationList$ in $PendingDequeues$}
        \If {$confirmationList.ts \prec Enq_{ts}$}
          \State $confirmationList.responses[j] = Safe$
        \EndIf
      \EndFor

      \State send $(EnqAck, i)$ to $p_j$
    \EndFunction

    \Function{Receive}{$EnqAck, j$ from $p_j$}
      \State $EnqCount \pluseq 1$
      \If {$EnqCount == n$} 
        \Return $EnqResponse$
      \EndIf
    \EndFunction
    \Statex
    \Statex \emph{(continues below)}
    \algstore{relaxedLines}
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Algorithm~\ref{alg:relaxed} continued: Handlers for $Dequeue$}
  \begin{algorithmic}[1]
    \algrestore{relaxedLines}
    \Function{Dequeue}{}
      \State $incrementTS(v_i)$
      \State let $Deq_{ts} = v_i$
      \If {$localQueue.peekByLabel(p_{i}) \neq \bot$}
        \State let $ret = localQueue.deqByLabel(p_i)$
        \State send $(Deq_f, ret, i, Deq_{ts})$ to all processes
        \State \Comment{\red{Don't we need to be returning to the user here?  Or does that happen after one round trip?}}
        \Statex \Comment{\red{Can't be here, as 0-duration op instances don't linearize.  But as written, our fast dequeues are no faster than slow dequeues, just choosing their return value sooner.}}
      \Else
        \State send $(Deq_s, null, i, Deq_{ts})$ to all processes
      \EndIf
    \EndFunction

    \Function{Receive}{($op, val, j, Deq_{ts})$ from $p_j$}
      \State $updateTS(v_i, Deq_{ts})$
      \State let $p_{invoker} = p_j$  \Comment{\red{As above, this seems extraneous.}}
      \If{$Deq_{ts}$ is not in $PendingDequeues$}
      \State $PendingDequeues.insertByTS(createList(Deq_{ts}, p_{invoker}$))
      \EndIf
      \If{$Deq_{ts} \neq 0$ and $Deq_{ts} \prec v_i$} \Comment{\red{As before, not sure this is comparing the right things.}}
        \State let $safetyFlag = Unsafe$
      \Else
        \State let $safetyFlag = Safe$
      \EndIf
      \State send $(op, val, safetyFlag, Deq_{ts}, i, p_{invoker})$ to all processes
    \EndFunction

    \Function{Receive}{$(op, val, safetyFlag, Deq_{ts}, i, p_{invoker})$ from $p_j$}
      \If{$Deq_{ts}$ not in $PendingDequeues$}
        \State $PendingDequeues.insertByTs(createList(Deq_{ts}, p_{invoker}))$
      \EndIf
      \State $propagateEarlierResponses(PendingDequeues)$ 
      \For{$(index, confirmationList)$ in $PendingDequeues$}
      \If{$confirmationList.ts == Deq_{ts}$}
          \State $confirmationList.responses[j] = safetyFlag$
        \EndIf 
        \If{$confirmationList.full()$ and not $confirmationList.handled$}
          \State $confirmationList.handled = True$ 
          \If {$op == Deq_f$ and $p_i \neq p_{invoker}$}
            \State $lQueue.remove(val)$
          \EndIf 
          \If {$op == Deq_f$} \Comment{\red{Only for $p_{invoker}$, yes?}}
            \State \Return
          \EndIf

          \If {$localQueue.peekByLabel(p_{invoker}) \neq \bot$}
          \State let $ret = lQueue.deqByLabel(p_{invoker})$
          \Else 
            \State let $pos$ = Number of $Unsafe$ flags in $confirmationList.responses$
            \State let $ret = lQueue.deqByIndex(pos)$
          \EndIf
          \State $labelElements(p_{invoker})$ \Comment{\red{Maybe should be in previous if?  Do we want to label if more elements are already labled for $p_{inv}$?  Or just go ahead and make sure it's stocked up?}} \Comment{\red{$labelElements()$ is not defined anywhere.}}
          \State $updateUnsafes(PendingDequeues, index)$
          \If{$p_i == p_{invoker}$}
            \State \Return $ret$
          \EndIf
        \EndIf 
      \EndFor
    \EndFunction
  \end{algorithmic}
\end{algorithm}



%%%%%%%%%%%%%%%
\subsection{Correctness}
For the K-OOO queue the same strucutre of a run R and a permutation $\pi$ is established. Lemmas 5.1, 5.2, and 5.4 remain unchanged, as the out of order behavior does not inherently adjust those behaviors of the queue structure. The Out of order behavior does not allow for the violation of real time order linearization, as it only allows for the structure to be accessed in an expanded manor. It also doesn't adjust how the there must be a matching response for any invocation or that there must be a corresponding enqueue for each dequeue invocations. 

However, given that the relaxation adjusts the structures of dequeues, we must further prove that with this adjusted behavior the local view of the queues are equivalent at the time of dequeue, and that elements, when dequeued are unique. 

\begin{lemma}
  The local view of the queue across processes are equivalent at the time of dequeue
\end{lemma}

\begin{proof}
  blee
\end{proof}

\begin{lemma}
    The claiming of elementy by individual processes will be agreed upon by all processes in the system
\end{lemma}

\begin{proof}
  blee
\end{proof}

\begin{lemma}
  Elements that are dequeued are unique
\end{lemma}

\begin{proof}
  bibliography
\end{proof}

%In the case where $k > n$, since the number of messages required to have a successful dequeue in bounds is less than the number of permitted dequeues ooo, dequeues can be instant. This information is based off of PUT THE PAPER NAME HERE which utilizes prior labeling of elements within the queue structure with a process that is allowed to instantly dequeue it.  Given each element is distinct and can only be dequeued once, the labeling must be distinct, in that each process agrees on the labelling that a given value in the queue receives. Whereas in PAPER NAME AGAIN this is done via a process of synchronization of clocks, we will be adjusting the labelling to occur at points when we can guarantee that the view of distinct processes agree, at the point of dequeueing. As we have discussed, at the point in which a process dequeues, the information prior to that dequeue, the prefix to the execution $\rho$ to that point will be the same for all processes. Given that all processes will agree on a select set of information at the same relative point in their executions, we can treat this as a sort of synchronization, and make decisions as to the labelling of elements at this point.

%For this relaxation to still follow the established queue definition, it must only return elements once. Therefore, we must prove that a) Each process dequeues each element only once and b) that elements removed are within the first $n$ elements within the queue.  Given that the labeling is agreed upon by every process, and done by calculations with an invariant, we can state that each process will agree on the elements that are ”fast dequeued” with respect to which process is allowed to ”fast dequeue” them. Given that processes agree which elements are ”reserved” for a fast dequeue by another process, we can furthermore state that a process will not slow dequeue an element that has been marked.

%Thus, elements can not be dequeued by processes that they have not been marked for, so we can guarantee that they will ONLY be dequeued by the assigned process.  By thje definition of the algorithm on line GIVE ME THE LINE NUMBER HERE COLE we can also state that the second criterion is satisfied, as at a point where a given process will attempt to fast dequeue, the position of the element that is attempting to be fast dequeued is checked, and the process is only allowed to execute the operation in the case where the element is within the legal bounds for dequeueing.


%%%%%%%%%%
\subsubsection{Less Relaxation than Number of Processes}
In the case in which the X of out of order is less than the number of processes,there isn’t an order of magnitude benefit to relaxation. The benefit of relaxation is that rather than forcing a wait of 2 times the maximum message delay, the wait can be cut to 2 times the (n-x+1)th slowest message delay. In a close connection system, the benefit is minimal as the difference between the max message delay and the (n-x+1)th is minimal, but in an actualized system with nonstandard message delays, this could have some substantial benefits.

%%%%%%%%%%%%%%%
\subsection{Complexity}

%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%
\section{Conclusion}

\bibliography{refs.bib}

\end{document}
o
